{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFR benchmark for Altmap vs Map Eq\n",
    "### Compare altmap to map eq using networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Using matplotlib backend: Qt5Agg\nPopulating the interactive namespace from numpy and matplotlib\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (AutoMinorLocator)\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# show plots in separate window\n",
    "%pylab\n",
    "# load helpers and wrappers\n",
    "%run helpers.py "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% imports\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from networkx.generators.community import LFR_benchmark_graph\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi_score\n",
    "\n",
    "# generate LFR benchmark graph + extract ground truth communities\n",
    "def generate_LFR_benchmark(N = 250, mu = 0.1):\n",
    "    \n",
    "    # LFR params N=10000\n",
    "    #params = {'max_degree':50, 'max_community':100, 'min_community':50, 'average_degree':20, 'tau1':3.0, 'tau2':1.5}\n",
    "    # LFR params N=5000\n",
    "    # params = {'max_degree':50, 'max_community':100, 'min_community':50, 'average_degree':20, 'tau1':2.5, 'tau2':1.5}\n",
    "    # LFR params N<1000\n",
    "    # params = {'max_degree':50, 'max_community':50, 'min_community':20, 'average_degree':10, 'tau1':3.0, 'tau2':1.5}\n",
    "    params = {'max_degree':int(0.1*N), 'max_community':int(0.1*N), 'min_community':20, 'average_degree':15, 'tau1':3.0, \n",
    "              'tau2':1.5}\n",
    "\n",
    "    max_degree = params['max_degree']\n",
    "    max_community = params['max_community']\n",
    "    min_community = params['min_community']\n",
    "    average_degree = params['average_degree']\n",
    "    tau1 = params['tau1'] # Power law exponent for the degree distribution \n",
    "    tau2 = params['tau2'] # Power law exponent for the community size distribution\n",
    "\n",
    "    # generate LFR benchmark graph\n",
    "    \n",
    "    G = LFR_benchmark_graph(N, tau1, tau2, mu, average_degree=average_degree, max_degree=max_degree, \n",
    "                            max_community=max_community, min_community=min_community, max_iters=200)\n",
    "    G = nx.convert_node_labels_to_integers(G, first_label=1)\n",
    "    \n",
    "    # extract ground truth communities from networkx graph object\n",
    "    communities_true = {}\n",
    "    num_communities = 0\n",
    "    for n in range(1,N+1):\n",
    "        if n in communities_true:\n",
    "            continue\n",
    "            \n",
    "        num_communities = num_communities + 1\n",
    "        community = G.nodes[n]['community']\n",
    "        node_ids = np.asarray(list(community))\n",
    "        node_ids = node_ids + 1 # have node labels >= 1\n",
    "        communities_true.update(dict.fromkeys(node_ids , num_communities))\n",
    "        \n",
    "    communities_true = OrderedDict(sorted(communities_true.items()))\n",
    "    \n",
    "    return G, communities_true\n",
    "\n",
    "# compute normalized mutual information between two partitions\n",
    "def compute_nmi(communities_true, communities_found):\n",
    "    labels_true = list(communities_true.values())\n",
    "    labels_found = list(communities_found.values())\n",
    "\n",
    "    return nmi_score(labels_true,labels_found, average_method='arithmetic')\n",
    "\n",
    "# LFR Benchmark\n",
    "# num_realizations .. number of network realizations for each parameter pair (mu, N)\n",
    "def run_benchmark(N_list, mu_list, num_realizations=10):\n",
    "    num_benchmarks = len(N_list) * len(mu_list) * num_realizations\n",
    "    benchmark_id = 0\n",
    "    \n",
    "    mean_nmi_infomap = np.zeros((len(mu_list), len(N_list)))\n",
    "    std_nmi_infomap = np.zeros((len(mu_list), len(N_list)))\n",
    "    mean_err_infomap = np.zeros((len(mu_list), len(N_list)))\n",
    "    std_err_infomap = np.zeros((len(mu_list), len(N_list)))\n",
    "    \n",
    "    mean_nmi_altmap = np.zeros((len(mu_list), len(N_list)))\n",
    "    std_nmi_altmap = np.zeros((len(mu_list), len(N_list)))\n",
    "    mean_err_altmap = np.zeros((len(mu_list), len(N_list)))\n",
    "    std_err_altmap = np.zeros((len(mu_list), len(N_list)))\n",
    "    \n",
    "    mean_nmi_altmap_init = np.zeros((len(mu_list), len(N_list)))\n",
    "    std_nmi_altmap_init = np.zeros((len(mu_list), len(N_list)))\n",
    "    mean_err_altmap_init = np.zeros((len(mu_list), len(N_list)))\n",
    "    std_err_altmap_init = np.zeros((len(mu_list), len(N_list)))\n",
    "    \n",
    "    for mu_idx, mu in enumerate(mu_list):\n",
    "        for N_idx, N in enumerate(N_list):\n",
    "            nmi_list_infomap = []\n",
    "            err_list_infomap = []\n",
    "            nmi_list_altmap = []\n",
    "            err_list_altmap = []\n",
    "            nmi_list_altmap_init = []\n",
    "            err_list_altmap_init = []\n",
    "            for realization in range(0, num_realizations):\n",
    "                benchmark_id = benchmark_id + 1\n",
    "                print(f'Starting benchmark {benchmark_id}/{num_benchmarks} for (N,mu) = ({N},{mu})\\n')\n",
    "                try:\n",
    "                    G, communities_true = generate_LFR_benchmark(N, mu)\n",
    "                    num_communities_true = max(communities_true.values()) - min(communities_true.values()) + 1\n",
    "                except nx.ExceededMaxIterations as err:\n",
    "                    print(f'Failed to generate network for (N,mu) = ({N},{mu}): ', err)\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                # test infomap\n",
    "                communities_found, num_communities_found = infomap(G, altmap=False)\n",
    "                print (f'Infomap found {num_communities_found} communities vs. {num_communities_true} ground truth '\n",
    "                       f'communities.\\n')\n",
    "                \n",
    "                nmi = compute_nmi(communities_true, communities_found)\n",
    "                nmi_list_infomap.append(nmi)\n",
    "                err_list_infomap.append(num_communities_found/num_communities_true - 1.0)\n",
    "                \n",
    "                # test altmap\n",
    "                communities_found, num_communities_found = infomap(G, altmap=True, update_inputfile=False)\n",
    "                print (f'Altmap found {num_communities_found} communities vs. {num_communities_true} ground truth '\n",
    "                       f'communities.\\n')\n",
    "                \n",
    "                nmi = compute_nmi(communities_true, communities_found)\n",
    "                nmi_list_altmap.append(nmi)\n",
    "                err_list_altmap.append(num_communities_found/num_communities_true - 1.0)\n",
    "                \n",
    "                # test altmap with init\n",
    "                communities_found, num_communities_found = infomap(G, altmap=True, init='sc', update_inputfile=False)\n",
    "                print (f'Altmap with init found {num_communities_found} communities vs. {num_communities_true} ground '\n",
    "                       f'truth communities.\\n')\n",
    "                \n",
    "                nmi = compute_nmi(communities_true, communities_found)\n",
    "                nmi_list_altmap_init.append(nmi)\n",
    "                err_list_altmap_init.append(num_communities_found/num_communities_true - 1.0)\n",
    "            \n",
    "            # check if benchmark generation succeeded\n",
    "            if nmi_list_infomap:\n",
    "                mean_nmi_infomap[mu_idx, N_idx] = np.mean(nmi_list_infomap)\n",
    "                std_nmi_infomap[mu_idx, N_idx] = np.std(nmi_list_infomap, ddof=1)\n",
    "                mean_err_infomap[mu_idx, N_idx] = np.mean(err_list_infomap)\n",
    "                std_err_infomap[mu_idx, N_idx] = np.std(err_list_infomap, ddof=1)\n",
    "                \n",
    "                mean_nmi_altmap[mu_idx, N_idx] = np.mean(nmi_list_altmap)\n",
    "                std_nmi_altmap[mu_idx, N_idx] = np.std(nmi_list_altmap, ddof=1)\n",
    "                mean_err_altmap[mu_idx, N_idx] = np.mean(err_list_altmap)\n",
    "                std_err_altmap[mu_idx, N_idx] = np.std(err_list_altmap, ddof=1)\n",
    "                \n",
    "                mean_nmi_altmap_init[mu_idx, N_idx] = np.mean(nmi_list_altmap_init)\n",
    "                std_nmi_altmap_init[mu_idx, N_idx] = np.std(nmi_list_altmap_init, ddof=1)\n",
    "                mean_err_altmap_init[mu_idx, N_idx] = np.mean(err_list_altmap_init)\n",
    "                std_err_altmap_init[mu_idx, N_idx] = np.std(err_list_altmap_init, ddof=1)\n",
    "            else:\n",
    "                mean_nmi_infomap[mu_idx, N_idx] = None\n",
    "                std_nmi_infomap[mu_idx, N_idx] = None\n",
    "                mean_err_infomap[mu_idx, N_idx] = None\n",
    "                std_err_infomap[mu_idx, N_idx] = None\n",
    "                \n",
    "                mean_nmi_altmap[mu_idx, N_idx] = None\n",
    "                std_nmi_altmap[mu_idx, N_idx] = None\n",
    "                mean_err_altmap[mu_idx, N_idx] = None\n",
    "                std_err_altmap[mu_idx, N_idx] = None\n",
    "                \n",
    "                mean_nmi_altmap_init[mu_idx, N_idx] = None\n",
    "                std_nmi_altmap_init[mu_idx, N_idx] = None\n",
    "                mean_err_altmap_init[mu_idx, N_idx] = None\n",
    "                std_err_altmap_init[mu_idx, N_idx] = None\n",
    "\n",
    "    \n",
    "    print(f'Finished benchmark successfully!\\n')\n",
    "    return mean_nmi_infomap, std_err_infomap, mean_err_infomap, std_nmi_infomap, \\\n",
    "           mean_nmi_altmap, std_err_altmap, mean_err_altmap, std_nmi_altmap, \\\n",
    "           mean_nmi_altmap_init, std_err_altmap_init, mean_err_altmap_init, std_nmi_altmap_init\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% helpers + wrappers\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "# LFR params\n",
    "N = 500\n",
    "mu = 0.3\n",
    "max_degree = 50\n",
    "max_community = 100\n",
    "min_community = 50\n",
    "average_degree = 20\n",
    "gamma = 2.5 # Power law exponent for the degree distribution \n",
    "beta = 1.5 # Power law exponent for the community size distribution\n",
    "max_iter = 100\n",
    "\n",
    "# generate LFR benchmark graph\n",
    "G = LFR_benchmark_graph(N, gamma, beta, mu, average_degree=average_degree, max_degree=max_degree, \n",
    "                        max_community=max_community, min_community=min_community, max_iters=max_iter)\n",
    "\n",
    "# plt.close('all')\n",
    "# plt.figure()\n",
    "# plt.title('Ground Truth Communities')\n",
    "# drawNetwork(G, communities_true, labels=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% param search\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0.1   0.425 0.75 ]\nStarting benchmark 1/3 for (N,mu) = (400,0.1)\n\n",
      "Infomap found 14 communities vs. 14 ground truth communities.\n\n",
      "Altmap found 14 communities vs. 14 ground truth communities.\n\n",
      "Altmap with init found 14 communities vs. 14 ground truth communities.\n\nStarting benchmark 2/3 for (N,mu) = (400,0.42500000000000004)\n\n",
      "Infomap found 1 communities vs. 14 ground truth communities.\n\n",
      "Altmap found 29 communities vs. 14 ground truth communities.\n\n",
      "Altmap with init found 24 communities vs. 14 ground truth communities.\n\nStarting benchmark 3/3 for (N,mu) = (400,0.75)\n\n",
      "Infomap found 1 communities vs. 16 ground truth communities.\n\n",
      "Altmap found 51 communities vs. 16 ground truth communities.\n\n",
      "Altmap with init found 29 communities vs. 16 ground truth communities.\n\nFinished benchmark successfully!\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "Ns = [400]\n",
    "# mus = np.linspace(0.15, 0.75, 10)\n",
    "mus = np.linspace(0.1, 0.75, 3)\n",
    "num_realizations = 1  \n",
    "print (mus)\n",
    "\n",
    "mean_nmi_infomap, std_err_infomap, mean_err_infomap, std_nmi_infomap, \\\n",
    "mean_nmi_altmap, std_err_altmap, mean_err_altmap, std_nmi_altmap, \\\n",
    "mean_nmi_altmap_init, std_err_altmap_init, mean_err_altmap_init, std_nmi_altmap_init = run_benchmark(Ns, mus, num_realizations=num_realizations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% run benchmarks\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7ff8c0ca1fd0>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "plt.close('all')\n",
    "fig, axs = plt.subplots(2,1,sharex=True)\n",
    "fig.suptitle(f'LFR benchmark, N = {2000} nodes')\n",
    "\n",
    "axs[0].plot(mus, mean_nmi_infomap[:,0], 'x--', linewidth=2, markersize=12, label='Infomap')\n",
    "axs[0].plot(mus, mean_nmi_altmap[:,0], '^--', linewidth=2, markersize=12, label='Altmap')\n",
    "axs[0].plot(mus, mean_nmi_altmap_init[:,0], 'o--', linewidth=2, markersize=12, label='Altmap with sc init')\n",
    "axs[0].plot([0.5, 0.5], [0,1], 'r')\n",
    "axs[0].grid()\n",
    "axs[0].set_xlabel('Mixing parameter $\\mu$')\n",
    "axs[0].set_ylabel('NMI')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(mus, std_nmi_infomap[:,0], 'x--', linewidth=2, markersize=12, label='Infomap')\n",
    "axs[1].plot(mus, std_nmi_altmap[:,0], '^--', linewidth=2, markersize=12, label='Altmap')\n",
    "axs[1].plot(mus, std_nmi_altmap_init[:,0], 'o--', linewidth=2, markersize=12, label='Altmap with sc init')\n",
    "axs[1].plot([0.5, 0.5], [0,np.max(np.max(std_nmi_infomap))], 'r')\n",
    "axs[1].grid()\n",
    "axs[1].set_xlabel('Mixing parameter $\\mu$')\n",
    "axs[1].set_ylabel('Standard deviation')\n",
    "axs[1].legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Plot mean nmi over mixing parameter\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7f14f0073d68>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 9
    }
   ],
   "source": [
    "plt.close('all')\n",
    "plt.figure()\n",
    "plt.title(f'LFR benchmark, N = {2000} nodes')\n",
    "\n",
    "min_err = np.min(np.min([mean_err_infomap, mean_err_altmap, mean_err_altmap_init]))\n",
    "max_err = np.max(np.max([mean_err_infomap, mean_err_altmap, mean_err_altmap_init]))\n",
    "\n",
    "\n",
    "plt.plot(mus, mean_err_infomap[:,0], 'x--', linewidth=2, markersize=12, label='Infomap')\n",
    "plt.plot(mus, mean_err_altmap[:,0], '^--', linewidth=2, markersize=12, label='Altmap')\n",
    "plt.plot(mus, mean_err_altmap_init[:,0], 'o--', linewidth=2, markersize=12, label='Altmap with sc init')\n",
    "plt.plot([0.5, 0.5], [min_err-0.2,max_err+0.2], 'r')\n",
    "plt.ylim([min_err-0.2,max_err+0.2])\n",
    "plt.grid()\n",
    "plt.xlabel('Mixing parameter $\\mu$')\n",
    "plt.ylabel('Mean relative error')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Plot mean nmi over mixing parameter\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}